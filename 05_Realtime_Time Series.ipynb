{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/root/spark-2.3.3-bin-hadoop2.7/\"\n",
    "CURRENCY_CONVERT = \"USD\"\n",
    "CURRENCY = \"bitcoin\"\n",
    "CURRENCY_SYMBOL = \"BTC\"\n",
    "BASE_URL = 'https://min-api.cryptocompare.com/data/price'\n",
    "CRYPTO_FOLDER = f\"data/crypto/{CURRENCY_SYMBOL}\"\n",
    "TWITTER_FOLDER = f\"data/twitter/{CURRENCY_SYMBOL}\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataframe):\n",
    "    del dataframe['time']\n",
    "\n",
    "    dataframe = dataframe.dropna() # drop missing values\n",
    "    y = dataframe.iloc[:, 1]  # Get label column \n",
    "    y = y.values\n",
    "\n",
    "    dataframe = dataframe.drop(columns='label') # Drop the label column\n",
    "    X = dataframe.values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def bootstrap_model(dataframe):\n",
    "\n",
    "    X_train, y_train = preprocess_data(dataframe)\n",
    "\n",
    "    my_params = {'objective':'reg:squarederror', 'verbose':False, 'n_estimators': 500, 'max_depth': 4}\n",
    "\n",
    "    model = xgb.XGBRegressor(**my_params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    model.save_model('btc_model.model')\n",
    "\n",
    "\n",
    "def predict_price(avg_score, prev_close, rolling_avg_val):\n",
    "    x_test = list((avg_score, prev_close, rolling_avg_val))\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.load_model('btc_model.model')\n",
    "    pred_price = model.predict(x_test)\n",
    "    return pred_price.tolist()[0]\n",
    "\n",
    "\n",
    "def retrain_model(avg_score, prev_close, rolling_avg_val, y_train_sample):\n",
    "    x_train_sample = list((avg_score, prev_close, rolling_avg_val))\n",
    "    x_train_sample = np.array(x_train_sample)    \n",
    "    x_train_sample = x_train_sample.reshape(1,3)\n",
    "    model = xgb.XGBRegressor()\n",
    "\n",
    "    model.fit(x_train_sample, np.array(y_train_sample).reshape(1,1), xgb_model = 'btc_model.model')\n",
    "    model.save_model('btc_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext, HiveContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder \\\n",
    "    .appName(\"model\").master('local').enableHiveSupport().getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_data=sc.read.option('header', True).option('inferSchema', True).csv(f\"{CRYPTO_FOLDER}/{CURRENCY}_currency_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data=sc.read.option('header', True).option('inferSchema', True).csv(f\"{TWITTER_FOLDER}/{CURRENCY}_tweets_grouped.csv\").withColumnRenamed('CreatedAt','time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond=[crypto_data.time == twitter_data.time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=twitter_data.join(crypto_data,cond,'left').select(twitter_data.time,twitter_data.score,crypto_data.close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag,col,avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w=Window.partitionBy().orderBy(col(\"time\"))\n",
    "data_train=data_train.select(\"*\", lag(\"close\").over(w).alias(\"previous_close\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = lambda i: i*(60*60)\n",
    "w = (Window.orderBy(col(\"time\").cast('long')).rangeBetween(-hour(1),0))\n",
    "data_train=data_train.withColumn('rolling_average',avg(data_train['previous_close']).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.na.drop(subset=[\"previous_close\"])\n",
    "data_train=data_train.na.drop(subset=[\"close\"])\n",
    "data_train = data_train.select(\"time\",\"score\",col(\"close\").alias(\"label\"),\"previous_close\",\"rolling_average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pd = data_train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_model(data_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['time', 'score', 'label', 'previous_close', 'rolling_average']\n",
    "\n",
    "time_stamp = time.strftime(\"%Y-%m-%d %H:%M:00\", time.gmtime())\n",
    "data_train.createOrReplaceTempView(\"table_df\")\n",
    "query_latest_rec = \"\"\"SELECT time, label FROM table_df ORDER BY time DESC limit 1\"\"\"\n",
    "latest_rec = sqlContext.sql(query_latest_rec)\n",
    "prev_close = latest_rec.collect()[0][\"label\"]\n",
    "latest_time = latest_rec.collect()[0][\"time\"]\n",
    "delta = timedelta(hours=1)\n",
    "window_time = latest_time - delta\n",
    "\n",
    "query_rolling_avg = \"SELECT previous_close FROM table_df WHERE TIME > \\'%s\\' AND TIME <= \\'%s\\' ORDER BY time DESC\" % (window_time, latest_time)\n",
    "avg_rec = sqlContext.sql(query_rolling_avg)\n",
    "#     avg_rec.show()\n",
    "\n",
    "new_close_row = sc.createDataFrame([(prev_close,)], ['previous_close'])\n",
    "#     new_close_row.show()\n",
    "\n",
    "avg_rec = avg_rec.union(new_close_row)\n",
    "#     avg_rec.show()\n",
    "\n",
    "rolling_avg_val = avg_rec.agg(avg(col('previous_close'))).collect()[0]['avg(previous_close)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Twitter Stream Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    sleep(60)\n",
    "    \n",
    "    stream = open(\"streamdata.txt\",\"r+\")\n",
    "    past_minute_data = stream.read().split(\",\")\n",
    "    stream.seek(0)\n",
    "    stream.truncate() # Deleting contents of file and getting ready for next \n",
    "\n",
    "    past_minute_data = list(map(float,past_minute_data[:-1]))\n",
    "    if( len(past_minute_data) ==0 ):\n",
    "        print(\"No data for last minute. Skipping..\")\n",
    "        continue\n",
    "        \n",
    "    last_minute_score = sum(past_minute_data)\n",
    "    \n",
    "    columns = ['time', 'score', 'label', 'previous_close', 'rolling_average']\n",
    "\n",
    "    time_stamp = datetime.datetime.fromtimestamp(time.mktime(time.gmtime()))\n",
    "    data_train.createOrReplaceTempView(\"table_df\")\n",
    "    query_latest_rec = \"\"\"SELECT time, label FROM table_df ORDER BY time DESC limit 1\"\"\"\n",
    "    latest_rec = sqlContext.sql(query_latest_rec)\n",
    "    prev_close = latest_rec.collect()[0][\"label\"]\n",
    "    latest_time = latest_rec.collect()[0][\"time\"]\n",
    "    delta = timedelta(hours=1)\n",
    "    window_time = latest_time - delta\n",
    "\n",
    "    query_rolling_avg = \"SELECT previous_close FROM table_df WHERE TIME > \\'%s\\' AND TIME <= \\'%s\\' ORDER BY time DESC\" % (window_time, latest_time)\n",
    "    avg_rec = sqlContext.sql(query_rolling_avg)\n",
    "\n",
    "    new_close_row = sc.createDataFrame([(prev_close,)], ['previous_close'])\n",
    "\n",
    "    avg_rec = avg_rec.union(new_close_row)\n",
    "\n",
    "    rolling_avg_val = avg_rec.agg(avg(col('previous_close'))).collect()[0]['avg(previous_close)']\n",
    "    \n",
    "    predicted_price = predict_price(last_minute_score, prev_close, rolling_avg_val)\n",
    "    clear_output()\n",
    "    print(\"Oracle predicts that the next minute \" + CURRENCY +\" value will be $\"+repr(predicted_price))\n",
    "    \n",
    "    contents = urllib.request.urlopen(\n",
    "        \"%s?fsym=%s&tsyms=%s\"%(BASE_URL,CURRENCY_SYMBOL,CURRENCY_CONVERT)\n",
    "    ).read()\n",
    "    json_string = contents.decode(\"utf-8\")\n",
    "    current_price = json.loads(json_string)[CURRENCY_CONVERT]\n",
    "    \n",
    "#     print(\"The current \"+ CURRENCY+ \"value is \" + repr(current_price))\n",
    "    \n",
    "#     print(\"Oracle updates itself to get beter next time..\\n\")\n",
    "    \n",
    "    retrain_model(last_minute_score, prev_close, rolling_avg_val,current_price)\n",
    "    newRow = sc.createDataFrame([(time_stamp, last_minute_score, current_price, prev_close, rolling_avg_val)], columns)\n",
    "    data_train = data_train.union(newRow)\n",
    "    \n",
    "    # Insert new row into Hive\n",
    "    sc.sql('INSERT INTO TABLE training_data values (\\'%s\\', %s, %s, %s, %s)' % (time_stamp, last_minute_score, current_price, prev_close, rolling_avg_val))\n",
    "    \n",
    "    oracle_db = open(\"oracle_db.txt\",\"a+\")\n",
    "    oracle_db.write(repr(predicted_price)+\",\"+repr(current_price)+\"\\n\")\n",
    "    oracle_db.close()\n",
    "    sc.sql(\"INSERT INTO TABLE oracle_db values (%s,%s)\" % (predicted_price, current_price))\n",
    "    plot_data = np.genfromtxt(\"oracle_db.txt\",delimiter= \",\")\n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "    plt.ion()\n",
    "    fig = plt.figure(figsize=(13,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # create a variable for the line so we can later update it\n",
    "    line1, = ax.plot(plot_data[:,0],'-o', color=\"red\", alpha=0.8,label ='Predicted Price')\n",
    "    line2, = ax.plot(plot_data[:,1],'-o',color=\"blue\", alpha=0.8,label ='Actual Price')\n",
    "    ax.legend()\n",
    "    #update plot label/title\n",
    "    plt.ylabel('USD ($)')\n",
    "    plt.xlabel('Time elapsed (mins)')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pd.to_csv('final30daydata.csv',index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
